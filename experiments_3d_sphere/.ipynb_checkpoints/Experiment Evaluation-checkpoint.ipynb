{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ef4072-920c-4a6b-b102-b08363f515b0",
   "metadata": {},
   "source": [
    "# Evaluation of Sphere Experiments\n",
    "\n",
    "This notebook will evaluate entropy, variance, iterations (AIPS) and coverage (ADTS) on the sphere experiements.  \n",
    "\n",
    "It has to be run from within the folder that holds all experiment folders that shall be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628db47a-61a1-4be2-ac08-e719864dca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy.spatial import distance_matrix\n",
    "from itertools import combinations, permutations, product\n",
    "import numba\n",
    "from  numba import jit,njit\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea089f22-57f2-4db0-b010-1c1babdb0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilliary functions to load, transform data and assign names appropriately\n",
    "\n",
    "# getting the data \n",
    "def get_folders():\n",
    "    folders = glob.glob(\"*/\")\n",
    "    folders = [fold for fold in folders]# if \"testng\" in fold.split(\"_\")]\n",
    "    return folders\n",
    "\n",
    "def get_experiments(folders):\n",
    "    return [f.split(\"___\")[-1][:-4]for f in folders if \"sphere\" in f and \"-1\" in f]\n",
    "    \n",
    "def split_by_experiment(folders,experiments):\n",
    "    experiment_map = {experiment: [] for experiment in experiments}\n",
    "    for f in folders:\n",
    "        for e in experiments:\n",
    "            if e in f:\n",
    "                experiment_map[e].append(f)\n",
    "                break\n",
    "    return experiment_map\n",
    "\n",
    "def to_df(folder, name):\n",
    "    files = glob.glob(os.path.join(folder,f\"*{name}*\"))\n",
    "    data = [np.loadtxt(f) for f in files]\n",
    "    try:\n",
    "        data = np.vstack(data)\n",
    "    except ValueError:\n",
    "        if \"reference\" in folder:\n",
    "            return pd.DataFrame() \n",
    "        else:\n",
    "            data = np.hstack(data)\n",
    "    if \"probs\" in name:\n",
    "        data = pd.DataFrame(data.T, columns=[\"probs\"])\n",
    "    elif \"iterations\" in name:\n",
    "        #try:        \n",
    "        data = pd.DataFrame(data.flatten(), columns=[\"iterations\"])\n",
    "        #except ValueError:\n",
    "        #    data = pd.DataFrame(data.flatten(),columns=[\"iterations\"])\n",
    "    else:        \n",
    "        data = pd.DataFrame(data,columns=[\"x\",\"y\",\"z\"])\n",
    "    experiment = folder.split(\"_\",1)[1][:-1].rsplit(\"_\",1)\n",
    "    data[\"experiment\"] =  experiment[0]\n",
    "    data[\"num\"] = experiment[1]\n",
    "    return data\n",
    "\n",
    "def get_data(folders,name):\n",
    "    return pd.concat([to_df(fold,name) for fold in folders])\n",
    "\n",
    "def get_ref_exp(data):\n",
    "    experiments = []\n",
    "    for exp in data[\"experiment\"].unique():\n",
    "        if \"reference\" in exp:\n",
    "            reference = exp\n",
    "        else:\n",
    "            experiments.append(exp)\n",
    "    sample_runs = data[\"num\"].unique().tolist()\n",
    "    return experiments, reference, sample_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4594f9-f205-4680-8a0c-e63286f95be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the metrics\n",
    "\n",
    "\n",
    "def experiment_as_arr(data,experiment, sample_run):\n",
    "    return data[(data[\"experiment\"] == experiment) & (data[\"num\"] == sample_run)][[\"x\",\"y\",\"z\"]].values\n",
    "\n",
    "\n",
    "def get_mean_nn_dist(data,data2,same=False):\n",
    "    dist = distance_matrix(data, data2)\n",
    "    if same:\n",
    "        dist = dist[dist != 0].reshape(len(data),len(data)-1)\n",
    "    try:\n",
    "        min_dist = dist.min(0)\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "        print(2)\n",
    "    return min_dist.mean()\n",
    "\n",
    "def resub_entropy(data):\n",
    "    # entropy for single dataset\n",
    "    return -np.log(data).sum() / len(data)\n",
    "\n",
    "def entropy(data):\n",
    "    # entropy evaluated over a dataset of multiple experiments\n",
    "    # will group data and evaluated entropy per group\n",
    "    data = data.groupby([\"experiment\",\"num\"]).agg(resub_entropy).reset_index()\n",
    "    data.rename(columns={\"probs\":\"entropy\"}, inplace=True)\n",
    "    return data\n",
    "    \n",
    "def coverage(data):\n",
    "    # old implementation of coverage\n",
    "    experiments, reference, sample_runs = get_ref_exp(data)\n",
    "    coverages = []\n",
    "    for i in sample_runs:\n",
    "        print(\"Evaluating coverage for sample run \",i)\n",
    "        data_ref = experiment_as_arr(data, reference,i)\n",
    "        coverage2 = lambda x: get_mean_nn_dist(x, data_ref)\n",
    "        cover = []\n",
    "        for exp in experiments:\n",
    "            arr = experiment_as_arr(data,exp,i)\n",
    "            if len(arr) == 0:\n",
    "                pdb.set_trace()\n",
    "            cover.append({\"coverage\":coverage2(arr),\"num\":i,\"experiment\":exp})\n",
    "        coverages.extend(cover)\n",
    "    return pd.DataFrame(coverages)\n",
    "    \n",
    "    \n",
    "def variance(data):\n",
    "    # evaluates variance of KDE estimates for a multiple experiments\n",
    "    data = data.groupby([\"experiment\",\"num\"]).agg(\"var\").reset_index()\n",
    "    data.rename(columns={\"probs\":\"variance\"}, inplace=True)\n",
    "    return data\n",
    "    \n",
    "\n",
    "def average_iterations(data):\n",
    "    data = data.groupby([\"experiment\",\"num\"]).agg(\"mean\").reset_index()\n",
    "    data.rename(columns={\"probs\":\"mean\"}, inplace=True)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b59e89-e9c6-435d-953c-ef45c92353ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(nogil=True)\n",
    "def dist(ref, samples):\n",
    "    # new coverage calculation\n",
    "    # faster with jit compilation numba\n",
    "    n,m = ref.shape\n",
    "    lowest = np.Inf\n",
    "    current = 0\n",
    "    min_vals = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        row = ref[i]\n",
    "        for k in range(n):\n",
    "            for j in range(m):\n",
    "                current += (row[j] - samples[k,j])**2\n",
    "            if current < lowest:\n",
    "                lowest = current\n",
    "            current = 0\n",
    "        min_vals[i] = lowest\n",
    "        lowest = np.Inf\n",
    "    return np.mean(min_vals)\n",
    "\n",
    "\n",
    "def get_data_for_dist(data, inputs,reference):\n",
    "    # helper that arranges data for parallel evaluation of coverage\n",
    "    experiment, sample_run = inputs\n",
    "    data_exp = experiment_as_arr(data, experiment, sample_run)\n",
    "    data_ref = experiment_as_arr(data, reference, sample_run)\n",
    "    cover = dist(data_ref, data_exp)\n",
    "    #print(cover)\n",
    "    return {\"experiment\": experiment, \"num\": sample_run, \"coverage\": cover}\n",
    "    \n",
    "def coverage_new(data):\n",
    "    # parallel evaluation of coverage for multiple experiments\n",
    "    experiments, reference, sample_runs = get_ref_exp(data)\n",
    "    coverages = []\n",
    "    cores = os.cpu_count()\n",
    "    #print(\"num cores: \",cores)\n",
    "    combs = product(experiments, sample_runs)\n",
    "    with ThreadPoolExecutor() as ex:\n",
    "        out = ex.map(lambda x: get_data_for_dist(data=data, reference=reference, inputs=x), combs)\n",
    "        df = pd.DataFrame(out)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73b601a5-c444-4a47-966c-33282d501683",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = get_folders()\n",
    "experiments = set(get_experiments(folders))\n",
    "experiment_map = split_by_experiment(folders,experiments)\n",
    "experiments = list(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8736b9ab-1588-4c9d-8752-3d0e64a38965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['off_center_connected', 'center_connected', 'off_center_disconnected']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9082f01-8b16-45be-81ad-2a97a76a9d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = experiment_map[experiments[0]]\n",
    "experiment_map[experiments[0]] = []\n",
    "# adjust off-center to own list entry\n",
    "for e in center:\n",
    "    if \"off\" in e:\n",
    "        experiment_map[\"off_center_connected\"].append(e)\n",
    "    else:\n",
    "        experiment_map[\"center_connected\"].append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d52ed2d3-3baa-4fbd-b8c2-d4a2eb0ef9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "off_center_connected\n",
      "finished loading variance and entropy\n",
      "finished loading coverages\n",
      "finished loading iterations\n",
      "center_connected\n",
      "finished loading variance and entropy\n",
      "finished loading coverages\n",
      "finished loading iterations\n",
      "off_center_disconnected\n",
      "finished loading variance and entropy\n",
      "finished loading coverages\n",
      "finished loading iterations\n"
     ]
    }
   ],
   "source": [
    "# acutual evaluation of metrics happens here\n",
    "# data is loaded and then evaluated\n",
    "\n",
    "\n",
    "variances = {}\n",
    "entropies = {}\n",
    "iterations = {}\n",
    "coverages = {}\n",
    "for exp, folders in experiment_map.items():\n",
    "    print(exp)\n",
    "    probs = get_data(folders,\"probs\")\n",
    "    variance_df = variance(probs)\n",
    "    entropy_df = entropy(probs)\n",
    "    print(\"finished loading variance and entropy\")\n",
    "    del probs\n",
    "\n",
    "    samples = get_data(folders,\"local*samples\")\n",
    "    coverage_df = coverage_new(samples)\n",
    "    print(\"finished loading coverages\")\n",
    "    del samples\n",
    "\n",
    "    num_iterations = get_data(folders,\"iterations\")\n",
    "    avg_iterations_df = average_iterations(num_iterations)\n",
    "    print(\"finished loading iterations\")\n",
    "    del num_iterations\n",
    "    \n",
    "    variances[exp] = variance_df\n",
    "    entropies[exp] = entropy_df\n",
    "    iterations[exp] = avg_iterations_df\n",
    "    coverages[exp] = coverage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c6b940-e152-46be-b075-b03aa1f11a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data\n",
    "\n",
    "for exp in experiment_map.keys():\n",
    "    suffix = exp\n",
    "    entropies[exp].to_csv(f\"entropy_{suffix}.csv\")\n",
    "    coverages[exp].to_csv(f\"coverage_{suffix}.csv\")\n",
    "    iterations[exp].to_csv(f\"iterations_{suffix}.csv\")\n",
    "    variances[exp].to_csv(f\"variance_{suffix}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f188f0c2-fee3-4282-8d40-b14f5c2cdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(word,exp,name): \n",
    "    if word in exp:\n",
    "        name = name + \"_\" + word\n",
    "    return name\n",
    "\n",
    "def rename(k):\n",
    "    name = \"\"\n",
    "    name = add_word(\"RRT\",k,name)\n",
    "    name = add_word(\"grid-walk\",k,name)\n",
    "    name = add_word(\"filter\",k,name)\n",
    "    name = add_word(\"multiple\",k,name)\n",
    "    name = add_word(\"single\",k,name)\n",
    "    name = add_word(\"ch\",k,name)\n",
    "    name = add_word(\"bandit\",k,name)\n",
    "    name = add_word(\"biased__\",k,name)\n",
    "    name = add_word(\"reference\",k,name)\n",
    "    \n",
    "    return name.strip(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc900d6-bb50-40aa-ba03-9cc4b2de4f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_to_data = {}\n",
    "metric_to_experiment_map = {}\n",
    "metric_to_experiment_map_rev = {}\n",
    "for exp in experiment_map.keys():\n",
    "    experiment_to_data[exp] = {\"variance\": variances[exp],\n",
    "    \"iterations\": iterations[exp],\n",
    "    \"entropy\": entropies[exp],\n",
    "    \"coverage\": coverages[exp]}\n",
    "    metric_to_experiment_map[exp] = {}\n",
    "    metric_to_experiment_map_rev[exp] = {}\n",
    "    for metric, data in experiment_to_data[exp].items():\n",
    "        experiment_map = {e:str(i) for i,e in enumerate(data.experiment.unique())} \n",
    "        metric_to_experiment_map[exp][metric] = experiment_map\n",
    "        metric_to_experiment_map_rev[exp][metric] = {v:k for k,v in experiment_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093062a4-dd85-48b9-93df-b199afd6384c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
